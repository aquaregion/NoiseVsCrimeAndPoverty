{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from numpy import linspace, zeros, cumsum, sqrt, ones, where, mean, std, unique, correlate, corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "import csv\n",
    "%pylab inline\n",
    "import pyfpgrowth\n",
    "import apyori\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Franco La Bruna & Yuchen Zheng\n",
    "###CSC 440 Final Project\n",
    "#Pattern Mining of New York City Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Note\n",
    "To run PyFPGrowth, you have to install the PyFPGrowth package on your computer. Check the folder \"fp-growth-master\" for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"\"\n",
    "df = pd.read_csv(directory+\"newfinal.csv\",sep = '\\t')\n",
    "df['Created_Date'] =pd.to_datetime(df.Created_Date)\n",
    "df['Neighborhood'] = 'NGHBR: '+df['Neighborhood'].astype(str)\n",
    "df = df.drop('City',1)\n",
    "year2010 = df.loc[df[\"Created_Date\"].dt.year == 2010] \n",
    "year2011 = df.loc[df[\"Created_Date\"].dt.year == 2011] \n",
    "year2012 = df.loc[df[\"Created_Date\"].dt.year == 2012] \n",
    "year2013 = df.loc[df[\"Created_Date\"].dt.year == 2013] \n",
    "year2014 = df.loc[df[\"Created_Date\"].dt.year == 2014] \n",
    "year2015 = df.loc[df[\"Created_Date\"].dt.year == 2015]\n",
    "#then you can check the size of for example, 2010's \n",
    "year2010.shape\n",
    "#save them into new csv files:\n",
    "directory1 = \"\"\n",
    "year2010.to_csv(directory1+'year2010.csv', sep = \"\\t\")\n",
    "year2011.to_csv(directory1+'year2011.csv', sep = \"\\t\")\n",
    "year2012.to_csv(directory1+'year2012.csv', sep = \"\\t\")\n",
    "year2013.to_csv(directory1+'year2013.csv', sep = \"\\t\")\n",
    "year2014.to_csv(directory1+'year2014.csv', sep = \"\\t\")\n",
    "year2015.to_csv(directory1+'year2015.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mine_patterns(data,min_sup,min_conf,filename):\n",
    "    # Helper function that finds frequent patterns and association rules\n",
    "    # and saves the results as .csv files.\n",
    "    thresh = min_sup*len(data)\n",
    "    pats = pyfpgrowth.find_frequent_patterns(data, thresh)\n",
    "    norm_pats = {k: v / len(data) for k, v in pats.items()}\n",
    "    patterns = sorted(norm_pats.items(), key=lambda x: x[1],reverse=True)\n",
    "    rules1 = pyfpgrowth.generate_association_rules(pats, 0.7)\n",
    "    rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "    directory = \"e:/Franco/Documents/DataMining/\"\n",
    "    with open(directory1+filename+'_patterns.csv', 'w', newline='\\n') as csvfile:\n",
    "        filewriter = csv.writer(csvfile)\n",
    "        filewriter.writerows(patterns)\n",
    "    with open(directory1+filename+'_rules.csv', 'w', newline='\\n') as csvfile:\n",
    "        filewriter = csv.writer(csvfile)\n",
    "        filewriter.writerows(rules_sorted1)\n",
    "    return patterns,rules_sorted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.applymap(str)\n",
    "total_data = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns,rules = mine_patterns(total_data,thresh,0.7,'Citywide')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Find Patterns by Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Bronx = df.loc[df[\"Borough\"] == 'BRONX']\n",
    "Bronx = Bronx.drop('Borough',1)\n",
    "bronx = Bronx.applymap(str)\n",
    "bronx1 = bronx.values.tolist()\n",
    "bronx_pats,bronx_rules = mine_patterns(bronx1,thresh,0.7,'Bronx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Brook = df.loc[df[\"Borough\"] == 'BROOKLYN']\n",
    "Brook = Brook.drop('Borough',1)\n",
    "brook = Brook.applymap(str)\n",
    "brook1 = brook.values.tolist()\n",
    "brook_pats,brook_rules = mine_patterns(brook1,thresh,0.7,'Brooklyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Man = df.loc[df[\"Borough\"] == 'MANHATTAN']\n",
    "Man = Man.drop('Borough',1)\n",
    "Man = Man.applymap(str)\n",
    "Man1 = Man.values.tolist()\n",
    "Man_pats,Man_rules = mine_patterns(Man1,thresh,0.7,'Manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "staten = df.loc[df[\"Borough\"] == 'STATEN ISLAND']\n",
    "staten = staten.drop('Borough',1)\n",
    "staten = staten.applymap(str)\n",
    "staten1 = staten.values.tolist()\n",
    "staten_pats,staten_rules = mine_patterns(staten1,thresh,0.7,'Staten_Island')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queens = df.loc[df[\"Borough\"] == 'QUEENS']\n",
    "queens = queens.drop('Borough',1)\n",
    "queens = queens.applymap(str)\n",
    "queens1 = queens.values.tolist()\n",
    "queens_pats,queens_rules = mine_patterns(queens1,thresh,0.7,'Queens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Find Patterns by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year2010 = year2010.drop('Year',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year2010 = year2010.applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          object\n",
       "Neighborhood        object\n",
       "Created_Date        object\n",
       "Complaint Type      object\n",
       "Descriptor          object\n",
       "Location Type       object\n",
       "Zip Code            object\n",
       "Borough             object\n",
       "Precinct            object\n",
       "Population          object\n",
       "Violence Binning    object\n",
       "Misd Binning        object\n",
       "Major Binning       object\n",
       "Nonmajor Binning    object\n",
       "Noise Binning       object\n",
       "Poverty Level       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year2010.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2010 = year2010.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh = 0.1\n",
    "min_sup = thresh*len(y2010)\n",
    "patterns2010 = pyfpgrowth.find_frequent_patterns(y2010, min_sup)\n",
    "p2010_norm = {k: v / len(y2010) for k, v in patterns2010.items()}\n",
    "patterns_sorted1 = sorted(p2010_norm.items(), key=lambda x: x[1],reverse=True)\n",
    "rules1 = pyfpgrowth.generate_association_rules(patterns2010, 0.7)\n",
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "with open(directory1+'patterns2010.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(patterns_sorted1)\n",
    "with open(directory1+'rules2010.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(rules_sorted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#year2011 = year2011.drop(year2011.columns[[0,1,2]],axis=1)\n",
    "year2011 = year2011.drop('Year',1)\n",
    "year2011 = year2011.applymap(str)\n",
    "y2011 = year2011.values.tolist()\n",
    "min_sup = thresh*len(y2011)\n",
    "patterns2011 = pyfpgrowth.find_frequent_patterns(y2011, min_sup)\n",
    "p2011_norm = {k: v / len(y2011) for k, v in patterns2011.items()}\n",
    "patterns_sorted1 = sorted(p2011_norm.items(), key=lambda x: x[1],reverse=True)\n",
    "rules1 = pyfpgrowth.generate_association_rules(patterns2011, 0.7)\n",
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "with open(directory1+'patterns2011.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(patterns_sorted1)\n",
    "with open(directory1+'rules2011.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(rules_sorted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#year2012 = year2012.drop(year2012.columns[[0,1,2]],axis=1)\n",
    "year2012 = year2012.drop('Year',1)\n",
    "year2012 = year2012.applymap(str)\n",
    "y2012 = year2012.values.tolist()\n",
    "min_sup = thresh*len(y2012)\n",
    "patterns2012 = pyfpgrowth.find_frequent_patterns(y2012, min_sup)\n",
    "p2012_norm = {k: v / len(y2012) for k, v in patterns2012.items()}\n",
    "patterns_sorted1 = sorted(p2012_norm.items(), key=lambda x: x[1],reverse=True)\n",
    "rules1 = pyfpgrowth.generate_association_rules(patterns2012, 0.7)\n",
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "with open(directory1+'patterns2012.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(patterns_sorted1)\n",
    "with open(directory1+'rules2012.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(rules_sorted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#year2013 = year2013.drop(year2013.columns[[0,1,2]],axis=1)\n",
    "year2013 = year2013.drop('Year',1)\n",
    "year2013 = year2013.applymap(str)\n",
    "y2013 = year2013.values.tolist()\n",
    "min_sup = thresh*len(y2013)\n",
    "patterns2013 = pyfpgrowth.find_frequent_patterns(y2013, min_sup)\n",
    "p2013_norm = {k: v / len(y2013) for k, v in patterns2013.items()}\n",
    "patterns_sorted1 = sorted(p2013_norm.items(), key=lambda x: x[1],reverse=True)\n",
    "rules1 = pyfpgrowth.generate_association_rules(patterns2013, 0.7)\n",
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "with open(directory1+'patterns2013.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(patterns_sorted1)\n",
    "with open(directory1+'rules2013.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(rules_sorted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#year2014 = year2014.drop(year2014.columns[[0,1,2]],axis=1)\n",
    "year2014 = year2014.drop('Year',1)\n",
    "year2014 = year2014.applymap(str)\n",
    "y2014 = year2014.values.tolist()\n",
    "min_sup = thresh*len(y2014)\n",
    "patterns2014 = pyfpgrowth.find_frequent_patterns(y2014, min_sup)\n",
    "p2014_norm = {k: v / len(y2014) for k, v in patterns2014.items()}\n",
    "patterns_sorted1 = sorted(p2014_norm.items(), key=lambda x: x[1],reverse=True)\n",
    "rules1 = pyfpgrowth.generate_association_rules(patterns2014, 0.7)\n",
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "with open(directory1+'patterns2014.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(patterns_sorted1)\n",
    "with open(directory1+'rules2014.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(rules_sorted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#year2015 = year2015.drop(year2015.columns[[0,1,2]],axis=1)\n",
    "year2015 = year2015.drop('Year',1)\n",
    "year2015 = year2015.applymap(str)\n",
    "y2015 = year2015.values.tolist()\n",
    "min_sup = thresh*len(y2015)\n",
    "patterns2015 = pyfpgrowth.find_frequent_patterns(y2015, min_sup)\n",
    "p2015_norm = {k: v / len(y2015) for k, v in patterns2015.items()}\n",
    "patterns_sorted1 = sorted(p2015_norm.items(), key=lambda x: x[1],reverse=True)\n",
    "rules1 = pyfpgrowth.generate_association_rules(patterns2015, 0.7)\n",
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)\n",
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "with open(directory1+'patterns2015.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(patterns_sorted1)\n",
    "with open(directory1+'rules2015.csv', 'w', newline='\\n') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerows(rules_sorted1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Based on Separate Boroughs\n",
    "This area has code for reading the separate boroughs, rather than a single file. Since we decided on using a single file, this section was not used for the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"e:/Franco/Documents/DataMining/\"\n",
    "bronx = pd.read_csv(directory+\"bronx1.csv\",sep = '\\t')\n",
    "staten = pd.read_csv(directory+\"staten.csv\",sep = '\\t')\n",
    "Man = pd.read_csv(directory+\"manhattan.csv\",sep = '\\t')\n",
    "Brook = pd.read_csv(directory+\"brooklyn.csv\",sep = '\\t')\n",
    "queens = pd.read_csv(directory+\"queens.csv\",sep = '\\t')\n",
    "df1 = [bronx,queens,Man,Brook,staten]\n",
    "df2 = pd.concat(df1)\n",
    "df2 = df2.drop('City',1)\n",
    "df2['Created_Date'] =pd.to_datetime(df2.Created_Date)\n",
    "df2['Neighborhood'] = 'NGHBR: '+df2['Neighborhood'].astype(str)\n",
    "df2 = df2.sort_values('Year')\n",
    "year2010 = df2.loc[df2[\"Created_Date\"].dt.year == 2010] \n",
    "year2011 = df2.loc[df2[\"Created_Date\"].dt.year == 2011] \n",
    "year2012 = df2.loc[df2[\"Created_Date\"].dt.year == 2012] \n",
    "year2013 = df2.loc[df2[\"Created_Date\"].dt.year == 2013] \n",
    "year2014 = df2.loc[df2[\"Created_Date\"].dt.year == 2014] \n",
    "year2015 = df2.loc[df2[\"Created_Date\"].dt.year == 2015]\n",
    "#then you can check the size of for example, 2010's \n",
    "year2010.shape\n",
    "#save them into new csv files:\n",
    "directory1 = \"e:/Franco/Documents/DataMining/\"\n",
    "year2010.to_csv(directory1+'year2010.csv', sep = \"\\t\")\n",
    "year2011.to_csv(directory1+'year2011.csv', sep = \"\\t\")\n",
    "year2012.to_csv(directory1+'year2012.csv', sep = \"\\t\")\n",
    "year2013.to_csv(directory1+'year2013.csv', sep = \"\\t\")\n",
    "year2014.to_csv(directory1+'year2014.csv', sep = \"\\t\")\n",
    "year2015.to_csv(directory1+'year2015.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.drop(df2[[0]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.applymap(str)\n",
    "total_data = df2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "bronx = bronx.drop('City',1)\n",
    "bronx = bronx.drop('Borough',1)\n",
    "bronx['Neighborhood'] = 'NGHBR: '+bronx['Neighborhood'].astype(str)\n",
    "bronx = bronx.applymap(str)\n",
    "bronx1 = bronx.values.tolist()\n",
    "bronx_pats,bronx_rules = mine_patterns(bronx1,thresh,0.7,'Bronx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "staten = staten.drop('City',1)\n",
    "staten = staten.drop('Borough',1)\n",
    "staten['Neighborhood'] = 'NGHBR: '+staten['Neighborhood'].astype(str)\n",
    "staten = staten.drop(staten.columns[[0]],axis=1)\n",
    "staten = staten.applymap(str)\n",
    "staten1 = staten.values.tolist()\n",
    "staten_pats,staten_rules = mine_patterns(staten1,thresh,0.7,'Staten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Man = Man.drop('City',1)\n",
    "Man = Man.drop('Borough',1)\n",
    "Man['Neighborhood'] = 'NGHBR: '+Man['Neighborhood'].astype(str)\n",
    "Man = Man.drop(Man.columns[[0,]],axis=1)\n",
    "Man = Man.applymap(str)\n",
    "Man1 = Man.values.tolist()\n",
    "Man_pats,Man_rules = mine_patterns(Man1,thresh,0.7,'Manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Brook = Brook.drop('City',1)\n",
    "Brook = Brook.drop('Borough',1)\n",
    "Brook['Neighborhood'] = 'NGHBR: '+Brook['Neighborhood'].astype(str)\n",
    "Brook = Brook.drop(Brook.columns[[0]],axis=1)\n",
    "brook = Brook.applymap(str)\n",
    "brook1 = brook.values.tolist()\n",
    "brook_pats,brook_rules = mine_patterns(brook1,thresh,0.7,'Brooklyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "queens = queens.drop('City',1)\n",
    "queens = queens.drop('Borough',1)\n",
    "queens['Neighborhood'] = 'NGHBR: '+queens['Neighborhood'].astype(str)\n",
    "queens = queens.drop(queens.columns[[0]],axis=1)\n",
    "queens = queens.applymap(str)\n",
    "queens1 = queens.values.tolist()\n",
    "queens_pats,queens_rules = mine_patterns(queens1,thresh,0.7,'Queens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_column(data,index):\n",
    "    label = data[0][index]\n",
    "    for i in range(1,len(data)):\n",
    "        data[i][index] = str(label)+str(data[i][index])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLines(data_file):\n",
    "    r = csv.reader(open(data_file, \"rt\", newline=''),delimiter='\\t')\n",
    "    return [l for l in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = \"e:/Franco/Downloads/\"\n",
    "transactions = readLines(directory+'revised_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1539583"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in transactions:\n",
    "    del row[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_sup = 0.15\n",
    "min_conf = 0.7\n",
    "patterns = list(apyori.apriori(transactions,min_support=min_sup,min_confidence=min_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresh = 0.10\n",
    "min_sup = thresh*len(transactions)\n",
    "patterns1 = pyfpgrowth.find_frequent_patterns(transactions, min_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patterns_sorted1 = sorted(patterns1.items(), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules1 = pyfpgrowth.generate_association_rules(patterns1, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rules_sorted1 = sorted(rules1.items(), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
